## 안녕하세요👋


저의 **기록 저장소**에 오신 것을 환영합니다! <br/>
이곳은 나만의 코드북을 만들기 위해 다양한 튜토리얼을 작성하고 기록하는 공간입니다. <br/>
현재는 시작 단계라 부족한 점이 많지만, 꾸준히 개선하고 새로운 내용을 추가할 예정입니다.<br/>
<br/>
튜토리얼은 기본적인 프로그래밍 개념부터 고급 기술까지 다양한 주제를 다룰 계획입니다. <br/>
성장하고 배우는 과정에서 여러분의 피드백과 제안은 언제나 환영합니다. <br/>
함께 배우고 발전해 나가는 유익한 공간이 되기를 바랍니다.<br/>
<br/>
> 더 많은 튜토리얼 보기 : https://github.com/seonm9119/tutorials/blob/main/README.md
---
### Tutorials
Category | Paper | Tutorial
:---: | :---: | :---:
SSL | [_Masked Autoencoders Are Scalable Vision Learners_](https://arxiv.org/abs/2111.06377) | [Masked Autoencoder](https://github.com/seonm9119/tutorials/blob/main/Masked%20Autoencoder(MAE).ipynb)
VFM | [_CvT: Introducing Convolutions to Vision Transformers_](https://arxiv.org/abs/2103.15808) | [Convolutional ViT](https://github.com/seonm9119/tutorials/blob/main/Convolutional%20ViT.ipynb)
VFM | [_Swin Transformer: Hierarchical Vision Transformer using Shifted Windows_](https://arxiv.org/abs/2103.14030) | [Swin Transformer](https://github.com/seonm9119/tutorials/blob/main/Swin.ipynb)
VFM | [_An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale_](https://arxiv.org/abs/2010.11929) | [Vision Transformer](https://github.com/seonm9119/tutorials/blob/main/Vision%20Transformer.ipynb)
Machine Translation| [_Attention Is All You Need_](https://arxiv.org/abs/1706.03762)| [Transformer](https://github.com/seonm9119/tutorials/blob/main/Transformer.ipynb)
NLP|[_BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding_](https://arxiv.org/abs/1810.04805)|[BERT](https://github.com/seonm9119/tutorials/blob/main/BERT.ipynb)|







