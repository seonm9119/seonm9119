## ì•ˆë…•í•˜ì„¸ìš”ğŸ‘‹


ì €ì˜ **ê¸°ë¡ ì €ì¥ì†Œ**ì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤! <br/>
ì´ê³³ì€ ë‚˜ë§Œì˜ ì½”ë“œë¶ì„ ë§Œë“¤ê¸° ìœ„í•´ ë‹¤ì–‘í•œ íŠœí† ë¦¬ì–¼ì„ ì‘ì„±í•˜ê³  ê¸°ë¡í•˜ëŠ” ê³µê°„ì…ë‹ˆë‹¤. <br/>
í˜„ì¬ëŠ” ì‹œì‘ ë‹¨ê³„ë¼ ë¶€ì¡±í•œ ì ì´ ë§ì§€ë§Œ, ê¾¸ì¤€íˆ ê°œì„ í•˜ê³  ìƒˆë¡œìš´ ë‚´ìš©ì„ ì¶”ê°€í•  ì˜ˆì •ì…ë‹ˆë‹¤.<br/>
<br/>
íŠœí† ë¦¬ì–¼ì€ ê¸°ë³¸ì ì¸ í”„ë¡œê·¸ë˜ë° ê°œë…ë¶€í„° ê³ ê¸‰ ê¸°ìˆ ê¹Œì§€ ë‹¤ì–‘í•œ ì£¼ì œë¥¼ ë‹¤ë£° ê³„íšì…ë‹ˆë‹¤. <br/>
ì„±ì¥í•˜ê³  ë°°ìš°ëŠ” ê³¼ì •ì—ì„œ ì—¬ëŸ¬ë¶„ì˜ í”¼ë“œë°±ê³¼ ì œì•ˆì€ ì–¸ì œë‚˜ í™˜ì˜í•©ë‹ˆë‹¤. <br/>
í•¨ê»˜ ë°°ìš°ê³  ë°œì „í•´ ë‚˜ê°€ëŠ” ìœ ìµí•œ ê³µê°„ì´ ë˜ê¸°ë¥¼ ë°”ëë‹ˆë‹¤.<br/>
<br/>
> ë” ë§ì€ íŠœí† ë¦¬ì–¼ ë³´ê¸° : https://github.com/seonm9119/tutorials/blob/main/README.md
---
### Tutorials
Category | Paper | Tutorial
:---: | :---: | :---:
SSL | [_Masked Autoencoders Are Scalable Vision Learners_](https://arxiv.org/abs/2111.06377) | [Masked Autoencoder](https://github.com/seonm9119/tutorials/blob/main/Masked%20Autoencoder(MAE).ipynb)
VFM | [_CvT: Introducing Convolutions to Vision Transformers_](https://arxiv.org/abs/2103.15808) | [Convolutional ViT](https://github.com/seonm9119/tutorials/blob/main/Convolutional%20ViT.ipynb)
VFM | [_Swin Transformer: Hierarchical Vision Transformer using Shifted Windows_](https://arxiv.org/abs/2103.14030) | [Swin Transformer](https://github.com/seonm9119/tutorials/blob/main/Swin.ipynb)
VFM | [_An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale_](https://arxiv.org/abs/2010.11929) | [Vision Transformer](https://github.com/seonm9119/tutorials/blob/main/Vision%20Transformer.ipynb)
Machine Translation| [_Attention Is All You Need_](https://arxiv.org/abs/1706.03762)| [Transformer](https://github.com/seonm9119/tutorials/blob/main/Transformer.ipynb)
NLP|[_BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding_](https://arxiv.org/abs/1810.04805)|[BERT](https://github.com/seonm9119/tutorials/blob/main/BERT.ipynb)|







